"""
Advanced RAG Evaluation Framework

Comprehensive evaluation system for RAG performance including:
- RAGAS metrics (Faithfulness, Answer Relevancy, Context Precision, Context Recall)
- Custom business metrics
- Human evaluation integration
- Automated evaluation pipelines
- Comparative analysis between models/configurations
"""

import asyncio
import json
import logging
import time
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import ollama

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class EvaluationResult:\n    \"\"\"Single evaluation result\"\"\"\n    query: str\n    generated_answer: str\n    ground_truth: Optional[str]\n    retrieved_contexts: List[str]\n    metrics: Dict[str, float]\n    metadata: Dict[str, Any]\n    timestamp: float\n\n@dataclass\nclass EvaluationSuite:\n    \"\"\"Complete evaluation suite results\"\"\"\n    name: str\n    results: List[EvaluationResult]\n    aggregate_metrics: Dict[str, float]\n    configuration: Dict[str, Any]\n    duration_seconds: float\n    timestamp: float\n\nclass RAGASEvaluator:\n    \"\"\"Implementation of RAGAS evaluation metrics\"\"\"\n    \n    def __init__(self, model_name: str = \"llama3.2:3b\", embedding_model: str = \"all-MiniLM-L6-v2\"):\n        self.model_name = model_name\n        self.client = ollama.Client()\n        self.embedding_model = SentenceTransformer(embedding_model)\n        \n    def calculate_faithfulness(self, answer: str, contexts: List[str]) -> float:\n        \"\"\"\n        Faithfulness: Measures if the answer is grounded in the given contexts\n        Score: [0, 1] where 1 means fully faithful\n        \"\"\"\n        if not contexts or not answer.strip():\n            return 0.0\n        \n        # Break answer into claims\n        claims = self._extract_claims(answer)\n        if not claims:\n            return 0.0\n        \n        faithful_claims = 0\n        for claim in claims:\n            if self._is_claim_supported(claim, contexts):\n                faithful_claims += 1\n        \n        return faithful_claims / len(claims)\n    \n    def calculate_answer_relevancy(self, question: str, answer: str) -> float:\n        \"\"\"\n        Answer Relevancy: Measures how relevant the answer is to the question\n        Score: [0, 1] where 1 means highly relevant\n        \"\"\"\n        if not question.strip() or not answer.strip():\n            return 0.0\n        \n        try:\n            # Generate multiple questions that the answer could address\n            generated_questions = self._generate_questions_from_answer(answer)\n            \n            if not generated_questions:\n                return 0.0\n            \n            # Calculate semantic similarity between original and generated questions\n            original_embedding = self.embedding_model.encode([question])\n            generated_embeddings = self.embedding_model.encode(generated_questions)\n            \n            similarities = cosine_similarity(original_embedding, generated_embeddings)[0]\n            return float(np.mean(similarities))\n            \n        except Exception as e:\n            logger.error(f\"Error calculating answer relevancy: {str(e)}\")\n            return 0.0\n    \n    def calculate_context_precision(self, question: str, contexts: List[str], answer: str) -> float:\n        \"\"\"\n        Context Precision: Measures if the retrieved contexts are relevant to the question\n        Score: [0, 1] where 1 means all contexts are relevant\n        \"\"\"\n        if not contexts:\n            return 0.0\n        \n        relevant_contexts = 0\n        for context in contexts:\n            if self._is_context_relevant(question, context, answer):\n                relevant_contexts += 1\n        \n        return relevant_contexts / len(contexts)\n    \n    def calculate_context_recall(self, ground_truth: str, contexts: List[str]) -> float:\n        \"\"\"\n        Context Recall: Measures if the ground truth can be attributed to the retrieved contexts\n        Score: [0, 1] where 1 means ground truth is fully covered\n        \"\"\"\n        if not ground_truth.strip() or not contexts:\n            return 0.0\n        \n        # Extract claims from ground truth\n        gt_claims = self._extract_claims(ground_truth)\n        if not gt_claims:\n            return 0.0\n        \n        covered_claims = 0\n        for claim in gt_claims:\n            if self._is_claim_supported(claim, contexts):\n                covered_claims += 1\n        \n        return covered_claims / len(gt_claims)\n    \n    def _extract_claims(self, text: str) -> List[str]:\n        \"\"\"Extract atomic claims from text\"\"\"\n        prompt = f\"\"\"Break down the following text into atomic claims (simple, factual statements). Each claim should be independent and verifiable.\n\nText: {text}\n\nAtomic claims:\n1.\"\"\"\n        \n        try:\n            response = self.client.generate(\n                model=self.model_name,\n                prompt=prompt,\n                options={'temperature': 0.1, 'num_predict': 300}\n            )\n            \n            claims = []\n            for line in response['response'].split('\\n'):\n                line = line.strip()\n                if line and any(line.startswith(str(i)) for i in range(1, 20)):\n                    claim = line.split('.', 1)[-1].strip()\n                    if claim and len(claim) > 10:\n                        claims.append(claim)\n            \n            return claims[:10]  # Limit to 10 claims\n            \n        except Exception as e:\n            logger.error(f\"Error extracting claims: {str(e)}\")\n            return [text]  # Fallback to original text\n    \n    def _is_claim_supported(self, claim: str, contexts: List[str]) -> bool:\n        \"\"\"Check if a claim is supported by the contexts\"\"\"\n        context_text = \"\\n\\n\".join(contexts)\n        \n        prompt = f\"\"\"Given the following context, is the claim supported by the information provided?\n\nContext:\n{context_text}\n\nClaim: {claim}\n\nAnswer with only 'YES' if the claim is clearly supported by the context, or 'NO' if it's not supported or unclear.\n\nAnswer:\"\"\"\n        \n        try:\n            response = self.client.generate(\n                model=self.model_name,\n                prompt=prompt,\n                options={'temperature': 0.0, 'num_predict': 10}\n            )\n            \n            answer = response['response'].strip().upper()\n            return answer.startswith('YES')\n            \n        except Exception as e:\n            logger.error(f\"Error checking claim support: {str(e)}\")\n            return False\n    \n    def _generate_questions_from_answer(self, answer: str, num_questions: int = 3) -> List[str]:\n        \"\"\"Generate questions that the answer could address\"\"\"\n        prompt = f\"\"\"Given the following answer, generate {num_questions} different questions that this answer could be responding to:\n\nAnswer: {answer}\n\nQuestions:\n1.\"\"\"\n        \n        try:\n            response = self.client.generate(\n                model=self.model_name,\n                prompt=prompt,\n                options={'temperature': 0.5, 'num_predict': 200}\n            )\n            \n            questions = []\n            for line in response['response'].split('\\n'):\n                line = line.strip()\n                if line and any(line.startswith(str(i)) for i in range(1, 10)):\n                    question = line.split('.', 1)[-1].strip()\n                    if question and len(question) > 10:\n                        questions.append(question)\n            \n            return questions[:num_questions]\n            \n        except Exception as e:\n            logger.error(f\"Error generating questions: {str(e)}\")\n            return []\n    \n    def _is_context_relevant(self, question: str, context: str, answer: str) -> bool:\n        \"\"\"Check if context is relevant to the question and useful for the answer\"\"\"\n        prompt = f\"\"\"Given a question and context, determine if the context is relevant and useful for answering the question.\n\nQuestion: {question}\n\nContext: {context}\n\nIs this context relevant and useful for answering the question? Answer with only 'YES' or 'NO'.\n\nAnswer:\"\"\"\n        \n        try:\n            response = self.client.generate(\n                model=self.model_name,\n                prompt=prompt,\n                options={'temperature': 0.0, 'num_predict': 10}\n            )\n            \n            answer = response['response'].strip().upper()\n            return answer.startswith('YES')\n            \n        except Exception as e:\n            logger.error(f\"Error checking context relevance: {str(e)}\")\n            return False\n\nclass BusinessMetricsEvaluator:\n    \"\"\"Custom business metrics for customer support RAG\"\"\"\n    \n    def __init__(self):\n        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n    \n    def calculate_completeness(self, question: str, answer: str) -> float:\n        \"\"\"How completely does the answer address the question?\"\"\"\n        # Define key aspects that should be covered\n        key_aspects = self._identify_key_aspects(question)\n        if not key_aspects:\n            return 1.0\n        \n        covered_aspects = 0\n        for aspect in key_aspects:\n            if self._is_aspect_covered(aspect, answer):\n                covered_aspects += 1\n        \n        return covered_aspects / len(key_aspects)\n    \n    def calculate_actionability(self, answer: str) -> float:\n        \"\"\"Does the answer provide actionable steps or clear guidance?\"\"\"\n        action_indicators = [\n            'step', 'click', 'go to', 'navigate', 'select', 'choose',\n            'follow', 'instructions', 'procedure', 'process', 'method'\n        ]\n        \n        answer_lower = answer.lower()\n        action_count = sum(1 for indicator in action_indicators if indicator in answer_lower)\n        \n        # Normalize by answer length\n        words = len(answer.split())\n        if words == 0:\n            return 0.0\n        \n        return min(1.0, action_count / (words / 50))  # Expected 1 action per 50 words\n    \n    def calculate_clarity(self, answer: str) -> float:\n        \"\"\"How clear and understandable is the answer?\"\"\"\n        # Simple heuristics for clarity\n        sentences = answer.split('.')\n        \n        if not sentences:\n            return 0.0\n        \n        # Average sentence length (shorter is often clearer)\n        avg_sentence_length = np.mean([len(s.split()) for s in sentences if s.strip()])\n        length_score = max(0, 1 - (avg_sentence_length - 15) / 20)  # Optimal ~15 words\n        \n        # Presence of structure (lists, numbers)\n        structure_score = 0.5\n        if any(char in answer for char in ['1.', '2.', 'â€¢', '-', '\\n']):\n            structure_score = 1.0\n        \n        # Technical jargon penalty\n        jargon_words = ['API', 'SQL', 'HTTP', 'JSON', 'XML', 'database', 'server']\n        jargon_count = sum(1 for word in jargon_words if word.lower() in answer.lower())\n        jargon_score = max(0, 1 - jargon_count / 10)\n        \n        return (length_score + structure_score + jargon_score) / 3\n    \n    def calculate_empathy(self, question: str, answer: str) -> float:\n        \"\"\"Does the answer show empathy and understanding?\"\"\"\n        empathy_phrases = [\n            'understand', 'sorry', 'help', 'assist', 'support',\n            'appreciate', 'thank', 'welcome', 'happy to', 'glad to'\n        ]\n        \n        answer_lower = answer.lower()\n        empathy_count = sum(1 for phrase in empathy_phrases if phrase in answer_lower)\n        \n        # Check for dismissive language\n        dismissive_phrases = ['just', 'simply', 'obviously', 'clearly', 'should know']\n        dismissive_count = sum(1 for phrase in dismissive_phrases if phrase in answer_lower)\n        \n        base_score = min(1.0, empathy_count / 3)\n        penalty = dismissive_count * 0.2\n        \n        return max(0, base_score - penalty)\n    \n    def _identify_key_aspects(self, question: str) -> List[str]:\n        \"\"\"Identify key aspects that should be addressed\"\"\"\n        question_lower = question.lower()\n        \n        aspects = []\n        \n        # Common question types and their aspects\n        if 'how to' in question_lower:\n            aspects.extend(['steps', 'procedure', 'requirements'])\n        if 'what is' in question_lower:\n            aspects.extend(['definition', 'explanation'])\n        if 'error' in question_lower or 'problem' in question_lower:\n            aspects.extend(['cause', 'solution', 'prevention'])\n        if 'cost' in question_lower or 'price' in question_lower:\n            aspects.extend(['pricing', 'payment'])\n        if 'cancel' in question_lower or 'refund' in question_lower:\n            aspects.extend(['process', 'timeline', 'conditions'])\n        \n        return list(set(aspects))\n    \n    def _is_aspect_covered(self, aspect: str, answer: str) -> bool:\n        \"\"\"Check if an aspect is covered in the answer\"\"\"\n        aspect_keywords = {\n            'steps': ['step', 'first', 'then', 'next', 'finally', 'process'],\n            'procedure': ['procedure', 'method', 'way', 'approach'],\n            'requirements': ['need', 'require', 'must', 'should', 'necessary'],\n            'definition': ['is', 'means', 'refers', 'definition'],\n            'explanation': ['because', 'reason', 'due to', 'explanation'],\n            'cause': ['cause', 'reason', 'due to', 'because'],\n            'solution': ['solution', 'fix', 'resolve', 'solve'],\n            'prevention': ['prevent', 'avoid', 'stop'],\n            'pricing': ['cost', 'price', 'fee', 'charge'],\n            'payment': ['pay', 'payment', 'billing', 'charge'],\n            'process': ['process', 'procedure', 'steps'],\n            'timeline': ['time', 'days', 'hours', 'when', 'timeline'],\n            'conditions': ['condition', 'terms', 'requirement', 'criteria']\n        }\n        \n        keywords = aspect_keywords.get(aspect, [aspect])\n        answer_lower = answer.lower()\n        \n        return any(keyword in answer_lower for keyword in keywords)\n\nclass RAGEvaluationFramework:\n    \"\"\"Comprehensive RAG evaluation framework\"\"\"\n    \n    def __init__(self, rag_engine, config: Dict[str, Any] = None):\n        self.rag_engine = rag_engine\n        self.config = config or {}\n        self.ragas_evaluator = RAGASEvaluator()\n        self.business_evaluator = BusinessMetricsEvaluator()\n        self.results_dir = Path(\"evaluation/results\")\n        self.results_dir.mkdir(parents=True, exist_ok=True)\n    \n    async def evaluate_single_query(\n        self, \n        query: str, \n        ground_truth: Optional[str] = None,\n        **rag_params\n    ) -> EvaluationResult:\n        \"\"\"Evaluate a single query\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Get RAG response\n            response = self.rag_engine.query(query, **rag_params)\n            \n            answer = response['answer']\n            sources = response.get('sources', [])\n            retrieved_contexts = [source.get('preview', source.get('content', '')) for source in sources]\n            \n            # Calculate RAGAS metrics\n            ragas_metrics = {}\n            ragas_metrics['faithfulness'] = self.ragas_evaluator.calculate_faithfulness(answer, retrieved_contexts)\n            ragas_metrics['answer_relevancy'] = self.ragas_evaluator.calculate_answer_relevancy(query, answer)\n            ragas_metrics['context_precision'] = self.ragas_evaluator.calculate_context_precision(query, retrieved_contexts, answer)\n            \n            if ground_truth:\n                ragas_metrics['context_recall'] = self.ragas_evaluator.calculate_context_recall(ground_truth, retrieved_contexts)\n            \n            # Calculate business metrics\n            business_metrics = {\n                'completeness': self.business_evaluator.calculate_completeness(query, answer),\n                'actionability': self.business_evaluator.calculate_actionability(answer),\n                'clarity': self.business_evaluator.calculate_clarity(answer),\n                'empathy': self.business_evaluator.calculate_empathy(query, answer)\n            }\n            \n            # Combine all metrics\n            all_metrics = {**ragas_metrics, **business_metrics}\n            \n            # Add system metrics\n            all_metrics['response_time_ms'] = response.get('generation_stats', {}).get('total_duration_ms', 0)\n            all_metrics['documents_retrieved'] = len(sources)\n            all_metrics['avg_similarity_score'] = response.get('retrieval_stats', {}).get('avg_score', 0)\n            \n            return EvaluationResult(\n                query=query,\n                generated_answer=answer,\n                ground_truth=ground_truth,\n                retrieved_contexts=retrieved_contexts,\n                metrics=all_metrics,\n                metadata={\n                    'rag_params': rag_params,\n                    'sources': sources,\n                    'response_data': response\n                },\n                timestamp=start_time\n            )\n            \n        except Exception as e:\n            logger.error(f\"Error evaluating query '{query}': {str(e)}\")\n            return EvaluationResult(\n                query=query,\n                generated_answer=\"\",\n                ground_truth=ground_truth,\n                retrieved_contexts=[],\n                metrics={'error': 1.0},\n                metadata={'error': str(e)},\n                timestamp=start_time\n            )\n    \n    async def evaluate_dataset(\n        self, \n        dataset: List[Dict[str, Any]], \n        suite_name: str,\n        **rag_params\n    ) -> EvaluationSuite:\n        \"\"\"Evaluate a complete dataset\"\"\"\n        start_time = time.time()\n        \n        logger.info(f\"Starting evaluation suite '{suite_name}' with {len(dataset)} queries\")\n        \n        results = []\n        for i, item in enumerate(dataset):\n            logger.info(f\"Evaluating query {i+1}/{len(dataset)}: {item['query'][:50]}...\")\n            \n            result = await self.evaluate_single_query(\n                query=item['query'],\n                ground_truth=item.get('ground_truth'),\n                **rag_params\n            )\n            results.append(result)\n            \n            # Progress update\n            if (i + 1) % 10 == 0:\n                logger.info(f\"Completed {i+1}/{len(dataset)} evaluations\")\n        \n        # Calculate aggregate metrics\n        aggregate_metrics = self._calculate_aggregate_metrics(results)\n        \n        suite = EvaluationSuite(\n            name=suite_name,\n            results=results,\n            aggregate_metrics=aggregate_metrics,\n            configuration={'rag_params': rag_params, 'dataset_size': len(dataset)},\n            duration_seconds=time.time() - start_time,\n            timestamp=start_time\n        )\n        \n        # Save results\n        self._save_evaluation_suite(suite)\n        \n        logger.info(f\"Evaluation suite '{suite_name}' completed in {suite.duration_seconds:.2f}s\")\n        return suite\n    \n    def _calculate_aggregate_metrics(self, results: List[EvaluationResult]) -> Dict[str, float]:\n        \"\"\"Calculate aggregate metrics from individual results\"\"\"\n        if not results:\n            return {}\n        \n        # Get all metric names\n        metric_names = set()\n        for result in results:\n            metric_names.update(result.metrics.keys())\n        \n        # Calculate means for each metric\n        aggregates = {}\n        for metric_name in metric_names:\n            values = []\n            for result in results:\n                if metric_name in result.metrics:\n                    value = result.metrics[metric_name]\n                    if isinstance(value, (int, float)) and not np.isnan(value):\n                        values.append(value)\n            \n            if values:\n                aggregates[f'{metric_name}_mean'] = np.mean(values)\n                aggregates[f'{metric_name}_std'] = np.std(values)\n                aggregates[f'{metric_name}_min'] = np.min(values)\n                aggregates[f'{metric_name}_max'] = np.max(values)\n        \n        # Calculate success rate\n        successful_queries = sum(1 for r in results if 'error' not in r.metrics)\n        aggregates['success_rate'] = successful_queries / len(results)\n        \n        return aggregates\n    \n    def _save_evaluation_suite(self, suite: EvaluationSuite):\n        \"\"\"Save evaluation suite to disk\"\"\"\n        timestamp = int(suite.timestamp)\n        filename = f\"{suite.name}_{timestamp}.json\"\n        filepath = self.results_dir / filename\n        \n        # Convert to serializable format\n        suite_dict = asdict(suite)\n        \n        with open(filepath, 'w') as f:\n            json.dump(suite_dict, f, indent=2, default=str)\n        \n        logger.info(f\"Evaluation results saved to {filepath}\")\n    \n    def compare_configurations(\n        self, \n        dataset: List[Dict[str, Any]],\n        configurations: List[Dict[str, Any]],\n        comparison_name: str\n    ) -> Dict[str, Any]:\n        \"\"\"Compare multiple RAG configurations\"\"\"\n        logger.info(f\"Starting configuration comparison '{comparison_name}'\")\n        \n        results = {}\n        \n        for i, config in enumerate(configurations):\n            config_name = config.get('name', f'config_{i}')\n            logger.info(f\"Testing configuration: {config_name}\")\n            \n            suite = asyncio.run(self.evaluate_dataset(\n                dataset=dataset,\n                suite_name=f\"{comparison_name}_{config_name}\",\n                **config.get('params', {})\n            ))\n            \n            results[config_name] = suite.aggregate_metrics\n        \n        # Generate comparison report\n        comparison_report = self._generate_comparison_report(results)\n        \n        # Save comparison\n        comparison_path = self.results_dir / f\"comparison_{comparison_name}_{int(time.time())}.json\"\n        with open(comparison_path, 'w') as f:\n            json.dump({\n                'configurations': configurations,\n                'results': results,\n                'report': comparison_report\n            }, f, indent=2, default=str)\n        \n        return {\n            'results': results,\n            'report': comparison_report,\n            'saved_to': str(comparison_path)\n        }\n    \n    def _generate_comparison_report(self, results: Dict[str, Dict[str, float]]) -> Dict[str, Any]:\n        \"\"\"Generate a comparison report between configurations\"\"\"\n        if len(results) < 2:\n            return {'error': 'Need at least 2 configurations to compare'}\n        \n        # Find best configuration for each metric\n        metrics = set()\n        for config_results in results.values():\n            metrics.update(config_results.keys())\n        \n        best_configs = {}\n        metric_comparisons = {}\n        \n        for metric in metrics:\n            if not metric.endswith('_mean'):\n                continue\n                \n            base_metric = metric.replace('_mean', '')\n            config_scores = {}\n            \n            for config_name, config_results in results.items():\n                if metric in config_results:\n                    config_scores[config_name] = config_results[metric]\n            \n            if config_scores:\n                # Higher is better for most metrics\n                best_config = max(config_scores, key=config_scores.get)\n                best_configs[base_metric] = {\n                    'config': best_config,\n                    'score': config_scores[best_config]\n                }\n                metric_comparisons[base_metric] = config_scores\n        \n        return {\n            'best_configurations': best_configs,\n            'metric_comparisons': metric_comparisons,\n            'summary': {\n                'total_metrics_compared': len(best_configs),\n                'configurations_tested': list(results.keys())\n            }\n        }\n\n# Example evaluation datasets\nSAMPLE_EVALUATION_DATASET = [\n    {\n        \"query\": \"How do I reset my password?\",\n        \"ground_truth\": \"To reset your password: 1. Go to the login page 2. Click 'Forgot Password' 3. Enter your email address 4. Check your email for a password reset link 5. Follow the instructions in the email to create a new password\"\n    },\n    {\n        \"query\": \"What payment methods do you accept?\",\n        \"ground_truth\": \"We accept the following payment methods: Credit cards (Visa, MasterCard, American Express, Discover), PayPal, Bank transfers (for enterprise customers), Apple Pay and Google Pay (mobile apps only)\"\n    },\n    {\n        \"query\": \"How do I cancel my subscription?\",\n        \"ground_truth\": \"To cancel your subscription: 1. Go to 'Subscription' in your account settings 2. Click 'Cancel Subscription' 3. Follow the prompts to confirm cancellation 4. Your access will continue until the end of your current billing period\"\n    },\n    {\n        \"query\": \"The application is running slowly, what should I do?\",\n        \"ground_truth\": \"If you're experiencing slow performance: 1. Check your internet connection speed 2. Clear your browser cache and cookies 3. Try using an incognito/private browser window 4. Disable browser extensions temporarily 5. Try a different browser 6. Contact support if issues persist\"\n    }\n]